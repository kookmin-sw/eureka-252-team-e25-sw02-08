[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/meKNgBF9)
# Welcome to 8팀

## 1️⃣ 팀원 소개

| **이름** | **전공** | **관심사** |
| --- | --- | --- |
| **박가현**| 소프트웨어학부 | 백엔드, 밴드, 여행 |
| **이주현** | 소프트웨어학부 | 프론트엔드, 운동, 여행 |
| **정찬민** | 소프트웨어학부 | 백엔드, 밴드, 건반 |
| **박세훈** | 소프트웨어학부 | 게임개발,라이엇 |
| **오윤택** | 소프트웨어학부 | 알고리즘, 밴드, 건반 |

유레카프로젝트 프로젝트 팀 생성을 축하합니다.
유레카프로젝트 프로젝트 팀의 제목과 팀원의 이름 및 관심사를 변경하세요.

### 팀 슬로건

Play the chord, Run the code.

### 팀 소개

22학번 2명이랑 25학번 3명으로 이루어져 있습니다.
한명뺴고 mbti가 다 i(내향형)입니다.


***

## 2️⃣ 공통된 관심사 : 밴드, 백엔드, 여행

***

## 3️⃣ 한학기 동안의 활동 내역 

- 기관/부서 인터뷰 ✔️  

- 현장 탐방 ✔️  

- 멘토링 ✔️  
  - 내 지도 교수 함게 만나기
  - 대학원 방문 및 선배 만나기

- 프로젝트 진행 ✔️  
  - 과거에 사람들이 상상한 미래
  - 그들이 만들어가는 세상
  - 우리가 상상한 미래
  - 우리가 그리는 미래 그리고 나

- 각오와 소감 나누기 ✔️

# Week03

### Misson 1
  정찬민 :
당시 사람들이 상상하거나 호기심으로만 여겼던 것들이 시간이 지나 실제로 실현된 것을 보고 놀라웠다. 이를 통해 미래의 엄청난 기술력들은 결국 사람들의 상상과 호기심에서 출발한다는 것을 느꼈고, 그렇기에 현재 우리가 품고 있는 상상 또한 머지않아 현실이 될 수 있겠다는 생각이 들었다.


  오윤택 :
대한민국은 1965년에는 거의 발전이 거의 되지않은 상태여서 만화가가 알 수 있는 미래기술이 거의 없다시피 했을텐데 관련 분야에대한 지식이 거의 없이 상상만으로 이렇게 미래에 실현이된 기술들을 예측을 했다는 것이 대단하다고 생각한다.


  박세훈 :
과거의 사람들이 말그대로 상상했던 미래들이 현재는 현실이 되어 있는 걸 보니 몹시 흥미로웠다. 당시에는 말그대로 허무맹랑한 기술들로 여겨졌지만 지금은 아주 당연해졌다는 점에서 그러면 우리의 지금 상상들이 좀 허무맹랑한 소리 같아도 미래에서는 현실이 될 수도 있지 않을까라는 희망을 갖게 되었다.


  박가현 :
어렸을 때 과학시간에 미래기술에 관련하여 그림그리기 대회를 한 적이 있는데 그때 하늘을 나는 자동차라던지 다른 행성에 살고있는 모습을 그린적이 있는데 머지않아 화성에 가서 살 날이 올수도 있다는 기사들이 나오고 현실이 될수도 있다고 생각하니 신기했다. 할머니 할아버지 세대는 초갓집에서 사셨던 분들도 계신데 지금 ai가 영상을 만들고 자동차가 무인으로 달리고 그런 모습들을 보면 신기할 거 같다는 생각이 든다


  이주현 :
1965년에 이 만화를 그릴 당시에는 해당 기술 중에 실현이 되겠다는 생각으로 그린 것이 아니라, “상용화되면 좋겠다.”, “되면 신기할 것 같다.”라는 생각이었을 것 같다. 하지만 불가능할 것 같았던 기술이 점차 상용화되는듯한 양상을 보이니까 기술이 빠르게 발전했다는 것을 실감할 수 있었다. 다만, 상용화되지 않은 기술 중에 움직이는 도로, 우주 여행 등은 기술적으로는 가능하나 환경적인 문제 때문에 상용화되지 않겠다는 생각도 들었다.

### Misson 2

  정찬민 : 
→ 현재 실현된 기술은? : 책상에 모니터가 달리고 펜으로 터치하는 기술,  핸드폰으로 물건을 결제하는 기술 (삼성페이,  애플페이), 핸드폰으로 지하철을 타는 기술, 화상 통화, 터치스크린, 서빙로봇, 사이버가수, VR
→느낀점 : 예전에 상상 속에서만 가능하다고 생각했던 기술들이 이제는 실제로 우리 생활 속에서 당연하게 사용되고 있다는 점이 놀라웠다. 특히 결제, 교통, 소통 같은 일상적인 부분에서 이런 기술들이 자리 잡으면서 생활이 훨씬 편리해지고 있다는 것을 느꼈다. 또, 사이버가수나 VR처럼 단순한 편리함을 넘어서 새로운 문화와 경험을 만들어내는 기술들을 보며 앞으로의 미래에는 지금 우리가 상상하는 것들도 머지않아 현실이 될 수 있겠다는 기대감을 가지게 되었다.


  박세훈 :
아직 기술적으로 부족하여 실현되지 못한 기술들이 있지만 그와 반대로 벌써 실현된 기술들도 몇 있다는 것을 알고 아직 실현되지 못한 기술들도 곧 있으면 실현될 수 있을 것만 같다는 희망을 갖게 되었다. 내가 상상해 봤었던 기술들도 있는 반면에 상상치도 못 해봤던 기술들도 나오는 걸 보면서 사람들의 창의력에 관하여 놀라웠다.


  오윤택 :
Mission2 실현된 기술
터치 스크린, 자율주행자동차,필기인식기술, ott서비스, 핸드폰으로결제,자동번역기
Mission2 느낀점
고등학교때 코로나때문에 했었던 원격수업이 90년대부터 아이디어가 나왔다는 것이 신기했고 실제로 상용화되기까지 오래걸렸다는 것도 의외였다. 또한 기술만있던 핸드폰 결제기술도 현재는 상용화가 되어서   대부분의 사람들이 쓰는 것을 보고 현재 아직 상용화 되지 않을 기술들도 언젠가 실제로 쓰이게 된다면 우리의 삶의 질을 더 높일 수 있을거라고 생각하게 되었다.


  박가현 :
  현재 실현 된 기술들은 아주 많습니다. 특히 자율주행 자동차의 경우 윤리의식의 문제로 인해 개발이나 발전이 더딜것이라고 생각했지만 지금 테슬라뿐 아니라 국내 자동차 회사에서도 자율주행을 일부분 적용하고 있습니다. 사실 제가 초등학생 때 생각했던 것인데, 상용화까지 정말 얼마 걸리지 않았다고 생각합니다. 그리고 코로나 이후로 ott서비스나, 실시간 원격 줌과 같은 기술들이 발전했다고 생각합니다. 코로나로 인해 기술 분야도 다른 형식으로 발전한 거 같아서 신기했다.


  이주현 :
코로나19 때, 온라인 수업이 진행되면서 현재 나의 입장에서는 매우 익숙한 기술이지만, 90년대 당시에는 실현되기를 원했던 사실에 놀랐다. OTT의 경우에도 익숙하게 보고 있지만, 당시에는 비디오를 이용해 보던 시대이기 때문에 기술 발전에 더욱이 놀라울 것 같았다. 그리고 마이크로소프트에서 공개한 미래비전 동영상에 대해서는 현재 완전히 실현된 것은 없으나, 스마트폰의 경우에는 화면 크기를 최대한 늘리려고 하는 모습을 보이고 있기 때문에 점차 시간이 지나면 실현될 것이라는 생각이 들었다.


# Week04
정찬민 : 
1. Why? -> 기존 입력장치인 키보드나 마우스, 모니터 등의 한계를 넘어 현실 세계와 디지털 세계를 자연스럽게 연결하기 위함.
What? -> 작은 프로젝트와 카메라를 활용하여 일상적인 사물 위에 디지털 정보를 직접 투샇하고, 사용자가 손가락 제스처를 통해 정보를 조작할 수 있는 새로운 인터페이스 장치.
How? -> 연구팀은 손가락에 장치를 부착하여 사용자가 특정 동작을 취하ㅕㄴ 이를 카메라가 인식하고, 프로젝터가 그 결과를 즉각적으로 투사하는 방식을 구현하였다. 이를 통해 책, 벽, 손바닥 등 어디서든 가상의 화면과 상호작용할 수 있다.

2. As-Is vs To-Be
키보드 -> 제스처 입력
마우스 -> 손가락의 움직임
모니터 -> 투사 가능한 가상 화면

3. 15년전에는 단순한 프로토타입 수준이었지만, 현재는 스마트폰 기반 AR, VR 기기, 스마트 글래스, 제스처 인식 기술 등으로 발전하여 다양한 분야에서 활용되고 있다. 


오윤택 :
1) 이 기술은 기존 입력 장치인 키보드, 마우스, 모니터의 한계를 넘어 물리적 세계와 디지털 세계를 자연스럽게 연결하기 위해 개발되었다. 사용자가 컴퓨터 앞에 고정되지 않고도 현실 속 사물과 디지털 정보를 실시간으로 주고받을 수 있도록, 정보를 물리적 환경 위에 직접 오버레이하고 손동작을 통해 제어할 수 있는 새로운 인터페이스를 제시한다.

2) 장치는 휴대용 프로젝터, 카메라, 센서, 미니 컴퓨터 등으로 구성된다. 카메라와 센서는 사용자의 손가락 위치와 움직임을 추적해 키보드와 마우스의 역할을 대신하고, 소형 프로젝터는 책, 벽, 손바닥 등 다양한 공간에 정보를 투사하여 모니터를 대체한다. 사용자는 손가락 제스처(예: 집기, 스크롤, 사진 찍는 동작)를 통해 직관적으로 입력을 할 수 있고, 장치는 이를 즉각 인식해 디지털 정보를 현실 공간에 반영한다.

3) 초기에는 단순한 프로토타입으로, 컬러 마커와 카메라를 이용해 제스처를 인식하고 프로젝터를 통해 결과를 출력하는 수준에 머물렀다. 그러나 현재는 인공지능 기반 비전 기술과 손 추적 센서를 활용하여 제스처 인식이 훨씬 정교해졌고, 출력 방식 또한 프로젝터 대신 AR 글래스와 같은 장치로 진화하였다. 실제로 눈의 움직임으로 문자를 입력하거나, 스마트워치가 손 씻는 동작을 감지하는 기능처럼 일상 속에서도 활용 가능한 기술이 등장했다.


박세훈 :
1)기술의 장벽을 낮추고 물리적 세계와 디지털 세계 간의 벽을 허물기 위해 개발되었다.
디지털 세계의 정보를 물리적 세계로 가져오거나 그 반대로 물리적 세계의 행동을 통해 디지털 세계에 영향을 끼치는 등 디지털 세계의 정보를 물리적 환경에 오버레이하고 실시간으로 상호작용할 수 있게 한다.
휴대용 장치와 카메라, 센서, 프로젝터 등과 같은 하드웨어와 현실적 세계의 행동을 인식하고 그로 인한 결과를 현실적 세계로 출력하는 소프트웨어의 통합을 통해 만들어졌다.

2)카메라와 센서를 통해 손가락의 위치와 움직임을 인식하여 마우스나 키보드의 역할을 수행하고 프로젝터를 통해 벽이나 손바닥 등과 같은 곳에 출력하여 모니터를 대체했다.

3)제스쳐 인식은 원래 컬러 마커와 카메라를 통해 이루어졌지만 현재는 AI 기반 비전과 손 추적 센서를 통해 더욱 더 심플해졌고 출력 기술은 프로젝터를 통해 현실의 공간에 출력했었지만 현재는 AR 기술을 통해 디지털 정보를 현실 세계에 출력하고 있다.


박가현 :
1. MIT Sixth Sense 연구팀이 만들고자 했던 것은?

Why:
기술을 통해 사람의 삶을 더 직관적이고 자연스럽게 만들고자 했다. 즉, 디지털 정보와 실제 세계의 경계를 없애 인간 중심의 컴퓨팅 환경을 구축하려는 목적이었다.
What:
사람의 손동작과 사물 인식을 통해 컴퓨터를 조작할 수 있는 ‘휴먼 인터페이스 시스템’. 사용자가 키보드나 마우스를 사용하지 않고도 현실 공간에서 정보를 불러오고 조작할 수 있는 웨어러블 장치(Sixth Sense).
How:
손가락에 색깔 마커를 붙이고, 목에 매단 카메라와 프로젝터를 이용해 사물 인식 + 제스처 추적 + 정보 투사를 결합했다. 이를 통해 종이에 그림을 그리면 디지털화되거나, 비행기표를 들면 실시간 정보가 나타나는 등 현실 속에서 디지털 정보가 자연스럽게 표현되도록 구현했다.

2. As-Is vs To-Be는 어떻게?
기존 컴퓨터(As-Is):
키보드: 물리적 자판 입력
마우스: 손의 움직임으로 화면 내 커서 제어
모니터: 정보는 화면 속에서만 시각적으로 확인

변화된 방식(To-Be):
키보드 → 손동작 인식으로 대체되어, 손가락 움직임만으로 문자나 명령 입력
마우스 → 제스처 제어로 전환되어, 사용자가 손가락으로 가리키거나 원을 그리면 명령 수행
모니터 → 프로젝션 디스플레이, 벽·책상·손바닥 등 현실 공간이 화면이 되어 정보가 투사됨

3. MIT Sixth Sense Project의 현재 기술 발전 모습

2009년 MIT에서 시작된 기술은 오늘날 AR(증강현실), VR(가상현실), 스마트글래스, 제스처 인식 시스템 등으로 발전했다.
예를 들어, Apple Vision Pro, Microsoft HoloLens, Meta Quest 같은 기기들은 당시 Sixth Sense가 꿈꾸었던 “현실 위에 정보를 덧입히는 세상”을 실현하고 있다.
또한 스마트폰의 카메라 인식 기능, 손동작 제어 UI 등도 그 연장선에 있다.
즉, Sixth Sense는 “상상의 현실화”의 시초로서 현재의 공간 컴퓨팅 시대를 여는 씨앗이 되었다.


이주현 :
1) Why? (왜?) → 현실 세계의 모든 사물과 정보를 디지털 정보와 자연스럽게 연결해서, 사람이 굳이 컴퓨터 앞에 앉지 않아도 언제 어디서든 "정보와 상호작용"할 수 있도록 하기 위함.
What? (무엇?) → 휴대용 프로젝터 + 카메라 + 미니 컴퓨터 + 센서로 구성된 웨어러블 장치. 사용자가 손 제스처를 하거나 물체를 가리키면, 카메라가 이를 인식하고, 프로젝터가 즉시 현실 공간 위에 디지털 정보를 투사하는 장치.
How? (어떻게?) → 목에 걸거나 착용하는 장치가 카메라로 손가락/사물을 추적 → 소형 프로젝터가 책, 벽, 손바닥 위에 화면을 투사 → 사용자는 손가락 제스처(예: 사진 찍는 동작)로 입력. 즉, 키보드·마우스·모니터 없이 제스처와 프로젝션을 통해 입출력이 가능해진다.

2) 기존 (As-Is) : 키보드 → 손으로 직접 타이핑, 마우스 → 클릭/드래그, 모니터 → 고정된 디스플레이
Sixth Sense (To-Be) : 손 제스처, 공중 타이핑 (손가락 추적), 손가락 움직임으로 화면 제어 (예: 집기 동작, 스크롤), 어디든 투사되는 화면 (벽, 종이, 손바닥 등)

3) 현재 투사되어 단순히 손가락이나 몸의 움직임을 이용해서 사물을 추적할거나 사진을 찍는 듯한 행위는 불가능하지만, 눈의 움직임을 이용해 문자를 입력하는 기술, 스마트워치의 경우에는 손을 씻는 듯한 행위를 하면 씻는 것으로 인식하는 등의 기술은 현재 구현이 된 상태이다. 따라서 MIT Sixth Sense 프로젝트의 아이디어는 계속해 계승, 발전되었다고 할 수 있다.


# Week05
<img width="1024" height="1024" alt="Image" src="https://github.com/user-attachments/assets/5b92ed54-346b-4874-a6d1-a5a99dae6047" />

<!-- 활동 사진 추가 예시 -->
<img src="https://pixnio.com/free-images/2017/08/14/2017-08-14-13-09-09-960x651.jpg?text=활동사진1" width="330" height="190"/>
<img src="https://pixnio.com/free-images/2017/08/14/2017-08-14-20-51-02-960x640.jpg?text=활동사진2" width="330" height="190"/>
<img src="https://pixnio.com/free-images/2017/08/15/2017-08-15-10-05-39-960x640.jpg?text=활동사진3" width="330" height="190"/>
<img width="594" height="842" alt="IMG_0051" src="https://github.com/user-attachments/assets/71e9b512-fafb-4a30-befb-48c278262fc4" />


***

## 4️⃣ 인상 깊은 활동

- 활동명 – 활동에 대한 간단한 설명과 배운 점을 작성  
- 예: 멘토링에서 실리콘밸리 현업 경험을 들을 수 있어 진로 방향 설정에 큰 도움이 되었다.  

***

## 5️⃣ 특별히 알아보고 싶은 것
- 예: 현장실습 제도
- 예: TOPCIT 정기평가
- 예: 졸업 후 진로(대학원/취업)

***

## 6️⃣ 활동을 마친 소감

🔗학번 이름  
> "소감 내용을 여기에 작성합니다."

🔗학번 이름  
> "소감 내용을 여기에 작성합니다."

🔗학번 이름  
> "소감 내용을 여기에 작성합니다."

🔗학번 이름  
> "소감 내용을 여기에 작성합니다."

🔗학번 이름  
> "소감 내용을 여기에 작성합니다."


## Markdown을 사용하여 내용꾸미기를 익히세요.

Markdown은 작문을 스타일링하기위한 가볍고 사용하기 쉬운 구문입니다. 여기에는 다음을위한 규칙이 포함됩니다.

```markdown
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
```

자세한 내용은 [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).

### Support or Contact

readme 파일 생성에 추가적인 도움이 필요하면 [도움말](https://help.github.com/articles/about-readmes/) 이나 [contact support](https://github.com/contact) 을 이용하세요.

